---
title: "PS4 v1.2: Spatial"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \usepackage{hyperref}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Tarini Dewan, tarinidewan
    - Partner 2 (name and cnet ID): Shreya Shravini, 
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: SS TD
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**": SS TD*  (1 point)
6. Late coins used this pset: 1 Late coins left after submission: 3
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

```{python}
import pandas as pd
import altair as alt
alt.data_transformers.disable_max_rows() 
alt.renderers.enable("png", ppi=250)
import numpy as np

import warnings 
warnings.filterwarnings('ignore')
```



## Download and explore the Provider of Services (POS) file (10 pts)

1. The variables are called:

- PRVDR_CTGRY_SBTYP_CD: Provider Category Subtype Code 
- PRVDR_CTGRY_CD: Provider Category Code
- PRVDR_NUM: CCN (CMS Certification Number)
- PGM_TRMNTN_CD: Termination Code
- ZIP_CD: ZIP Code

2. 


```{python}
import pandas as pd

# Load the dataset
file_path = "C:/Users/Shreya Work/OneDrive/Documents/GitHub/problem-set-4-shreya-and-tarini/pos2016.csv"  
pos_data = pd.read_csv(file_path)

# Filter for short-term hospitals: provider type code 01 and subtype code 01
short_term_hospitals = pos_data[
    (pos_data['PRVDR_CTGRY_CD'] == 1) & (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)
]

# Count the number of hospitals
num_hospitals = short_term_hospitals.shape[0]
print(f"Number of short-term hospitals: {num_hospitals}")



```


3. 

``` {python}

import matplotlib.pyplot as plt

# Define file paths for each year's dataset
file_paths = {
    2016: "C:/Users/Shreya Work/OneDrive/Documents/GitHub/problem-set-4-shreya-and-tarini/pos2016.csv",
    2017: "C:/Users/Shreya Work/OneDrive/Documents/GitHub/problem-set-4-shreya-and-tarini/pos2017.csv",
    2018: "C:/Users/Shreya Work/OneDrive/Documents/GitHub/problem-set-4-shreya-and-tarini/pos2018.csv",
    2019: "C:/Users/Shreya Work/OneDrive/Documents/GitHub/problem-set-4-shreya-and-tarini/pos2019.csv"
}


# Initialize an empty list to store filtered DataFrames
hospital_data = []

# Loop through each file path, load data, filter, and append to list
# Adjusted code to handle encoding
for year, path in file_paths.items():
    data = pd.read_csv(path, encoding='ISO-8859-1')  # Change encoding if needed
    short_term_hospitals = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    short_term_hospitals['Year'] = year  # Add a year column for tracking
    hospital_data.append(short_term_hospitals)


# Concatenate all years' data into a single DataFrame
combined_data = pd.concat(hospital_data, ignore_index=True)

# Plot the number of observations by year
obs_by_year = combined_data['Year'].value_counts().sort_index()
obs_by_year.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel("Year")
plt.ylabel("Number of Short-Term Hospitals")
plt.title("Number of Short-Term Hospitals by Year (2016-2019)")
plt.show()

```



```{python}
# import pos2016.csv
pos2016 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2016.csv')

# create a year column
pos2016['YEAR'] = '2016'
  
# subset data to short-term hospitals
pos2016_hosp = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)].reset_index(drop = True)
pos2016_hosp
```  

    a. There are 7,245 hospitals reported in the data. This number seems greater than the actual number of hospitals.

```{python}
pos2016_hosp.shape
pos2016_hosp['PRVDR_NUM'].nunique()
```

    b. According to the American Hospital Association, there were a total of 5,534 hospitals in FY 2016. The mismatch could be because the number in our data includes hospitals that have merged or closed (CMS = 01).
    
    Attribution: https://www.aha.org/system/files/2018-01/Fast%20Facts%202018%20pie%20charts.pdf

```{python}
# check termination status
pos2016_hosp['PGM_TRMNTN_CD'].value_counts()
```

3. 

```{python}
# pos2017.csv
pos2017 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2017.csv')
pos2017['YEAR'] = '2017' # year column
pos2017_hosp = pos2017[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# pos2018.csv
pos2018 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2018.csv', encoding = 'latin1')
pos2018['YEAR'] = '2018' # year column
pos2018_hosp = pos2018[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# pos2019.csv
pos2019 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2019.csv', encoding = 'latin1')
pos2019['YEAR'] = '2019' # year column
pos2019_hosp = pos2019[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# append datasets
pos_df = pd.concat([pos2016_hosp, pos2017_hosp, pos2018_hosp, pos2019_hosp])

# plot the number of observations in your dataset by year
alt.Chart(pos_df).mark_bar().encode(
    x = alt.X('YEAR:N', title = 'Year'),
    y = alt.Y('count(PRVDR_CTGRY_SBTYP_CD):Q', title = 'Count of Hospitals')
).properties(
    title = 'Distribution of Hospitals Over Time'
)
```

4. 
    a.

```{python}
# dropping duplicate CMS numbers across years
pos_df_uniq = pos_df.drop_duplicates(subset = ['PRVDR_NUM', 'YEAR'], keep = 'first')

# plot the number of unique hospitals in your dataset by year
alt.Chart(pos_df_uniq).mark_bar().encode(
    x = alt.X('YEAR:N', title = 'Year'),
    y = alt.Y('count(PRVDR_CTGRY_SBTYP_CD):Q', title = 'Count of Unique Hospitals')
).properties(
    title = 'Distribution of Unique Hospitals Over Time'
)
```

    b. The plots are the same which tells us that the dataset is structured such that each row represents a unique hospital per year, with no repeated entries for any hospital within the same year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are:

    - .dbf: The dBASE table that stores the attribute information of features; data is stored in an array with multiple records and fields (has attribute info)
    - .prj: The file that stores the coordinate system information 
    - .shp: The main file that stores the feature geometry (points, lines, polygons)
    - .shx: The index file that stores the index of the feature geometry (has positional index)
    - .xml: file format that stores information about the shapefile

    # Attribution: https://desktop.arcgis.com/en/arcmap/latest/manage-data/shapefiles/shapefile-file-extensions.htm
    
    b. The size for each of the files is given in the output below.

```{python}
import os

# define the folder path
folder_path = '/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/gz_2010_us_860_00_500k'

# get file size
for file_name in os.listdir(folder_path):
    file_path = os.path.join(folder_path, file_name)
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert size to MB
    print(f"File Name: {file_name}, File Size: {file_size_mb:.2f} MB")
```

2. The number of hospitals per zip code is given in the output below.

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

# loading .shp file
filepath = "/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
texas_df = gpd.read_file(filepath)

# filtering data to Texas zip codes
texas_df['Texas'] = texas_df['ZCTA5'].apply(lambda x: 1 if x.startswith(('75', '76', '77', '78', '79')) else 0)
texas_df = texas_df[texas_df['Texas'] == 1]

# convert zip codes in both datasets to string
texas_df['ZCTA5'] = texas_df['ZCTA5'].astype(str)
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].fillna(0) # account for NAs in pos2016 zip code data
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].astype(int).astype(str)

# merge both datasets by zip code
hosp_tex_2016 = pd.merge(texas_df, pos2016, how = 'inner', left_on = 'ZCTA5', right_on = 'ZIP_CD')

# number of hospitals in Texas per zip code 2016
print(hosp_tex_2016['ZCTA5'].value_counts())

# 
hosp_per_zip = hosp_tex_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# merge with spatial geometry data for zip codes
hospitals_geo = hosp_tex_2016[['ZIP_CD', 'geometry']].drop_duplicates().merge(
    hosp_per_zip, on='ZIP_CD', how='left').fillna(0)

# choropleth of hospitals by zip code
plt.figure(figsize=(20, 16), dpi = 1000)
hospitals_geo.plot(column='hospital_count', cmap='OrRd', legend=True, edgecolor='black').set_axis_off()
plt.title("Number of Hospitals per ZIP Code in Texas 2016")
plt.show()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
