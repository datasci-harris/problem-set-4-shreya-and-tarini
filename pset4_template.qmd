---
title: "PS4 v1.2: Spatial"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \usepackage{hyperref}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Tarini Dewan, tarinidewan
    - Partner 2 (name and cnet ID): Shreya Shravini, 
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: SS TD
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**": SS TD*  (1 point)
6. Late coins used this pset: 1 Late coins left after submission: 3
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

```{python}
import pandas as pd
import altair as alt
alt.data_transformers.disable_max_rows() 
alt.renderers.enable("png", ppi=250)
import numpy as np

import warnings 
warnings.filterwarnings('ignore')
```



## Download and explore the Provider of Services (POS) file (10 pts)

1. The variables are called:

- PRVDR_CTGRY_SBTYP_CD: Provider Category Subtype Code 
- PRVDR_CTGRY_CD: Provider Category Code
- PRVDR_NUM: CCN (CMS Certification Number)
- PGM_TRMNTN_CD: Termination Code
- ZIP_CD: ZIP Code


2.



```{python}
# import pos2016.csv
pos2016 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2016.csv')

# create a year column
pos2016['YEAR'] = '2016'
  
# subset data to short-term hospitals
pos2016_hosp = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)].reset_index(drop = True)
pos2016_hosp
```  

    a. There are 7,245 hospitals reported in the data. This number seems greater than the actual number of hospitals.

```{python}
pos2016_hosp.shape
pos2016_hosp['PRVDR_NUM'].nunique()
```

    b. According to the American Hospital Association, there were a total of 5,534 hospitals in FY 2016. The mismatch could be because the number in our data includes hospitals that have merged or closed (CMS = 01).
    
    Attribution: https://www.aha.org/system/files/2018-01/Fast%20Facts%202018%20pie%20charts.pdf

```{python}
# check termination status
pos2016_hosp['PGM_TRMNTN_CD'].value_counts()
```

3. 

```{python}
# pos2017.csv
pos2017 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2017.csv')
pos2017['YEAR'] = '2017' # year column
pos2017_hosp = pos2017[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2017['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# pos2018.csv
pos2018 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2018.csv', encoding = 'latin1')
pos2018['YEAR'] = '2018' # year column
pos2018_hosp = pos2018[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2018['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# pos2019.csv
pos2019 = pd.read_csv('/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/pos2019.csv', encoding = 'latin1')
pos2019['YEAR'] = '2019' # year column
pos2019_hosp = pos2019[(pos2017['PRVDR_CTGRY_CD'] == 1) & (pos2019['PRVDR_CTGRY_SBTYP_CD'] == 1)] # subsetting data

# append datasets
pos_df = pd.concat([pos2016_hosp, pos2017_hosp, pos2018_hosp, pos2019_hosp])

# plot the number of observations in your dataset by year
alt.Chart(pos_df).mark_bar().encode(
    x = alt.X('YEAR:N', title = 'Year'),
    y = alt.Y('count(PRVDR_CTGRY_SBTYP_CD):Q', title = 'Count of Hospitals')
).properties(
    title = 'Distribution of Hospitals Over Time'
)
```

4. 
    a.

```{python}
# dropping duplicate CMS numbers across years
pos_df_uniq = pos_df.drop_duplicates(subset = ['PRVDR_NUM', 'YEAR'], keep = 'first')

# plot the number of unique hospitals in your dataset by year
alt.Chart(pos_df_uniq).mark_bar().encode(
    x = alt.X('YEAR:N', title = 'Year'),
    y = alt.Y('count(PRVDR_CTGRY_SBTYP_CD):Q', title = 'Count of Unique Hospitals')
).properties(
    title = 'Distribution of Unique Hospitals Over Time'
)
```

    b. The plots are the same which tells us that the dataset is structured such that each row represents a unique hospital per year, with no repeated entries for any hospital within the same year. 

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
# Filter data for hospitals that were active in 2016
active_2016 = combined_data[(combined_data['Year'] == 2016) & (combined_data['PGM_TRMNTN_CD'] == 'A')]

# Initialize an empty list to record suspected closures
suspected_closures = []

# Loop over the active hospitals in 2016
for index, hospital in active_2016.iterrows():
    # Get the CMS Certification Number to track across years
    cms_cert_num = hospital['PRVDR_NUM']
    facility_name = hospital['FAC_NAME']
    zip_code = hospital['ZIP_CD']
    
    # Track if the hospital appears as active in each subsequent year
    active_in_later_years = False
    closure_year = None

    # Check 2017, 2018, and 2019
    for year in [2017, 2018, 2019]:
        # Filter for this hospital in the given year
        hospital_year_data = combined_data[(combined_data['Year'] == year) & (combined_data['PRVDR_NUM'] == cms_cert_num)]
        
        if hospital_year_data.empty:
            # If hospital does not appear at all, record the closure year
            closure_year = year
            break
        elif not any(hospital_year_data['PGM_TRMNTN_CD'] == 'A'):
            # If hospital appears but is not active, record the closure year
            closure_year = year
            break
        else:
            # If hospital is active, continue to next year
            active_in_later_years = True
    
    # If hospital was not active through 2019, add to suspected closures
    if not active_in_later_years:
        suspected_closures.append({
            'Facility Name': facility_name,
            'ZIP Code': zip_code,
            'Year of Suspected Closure': closure_year
        })

# Convert suspected closures to a DataFrame for easy viewing and analysis
closures_df = pd.DataFrame(suspected_closures)

# Output the number of suspected closures
num_closures = len(closures_df)
print(f"Number of hospitals suspected to have closed by 2019: {num_closures}")
closures_df

```

2. 

```{python}
sorted_closures_df = closures_df.sort_values(by='FAC_NAME') 

# Display the names and year of suspected closure for the first 10 rows
first_10_closures = sorted_closures_df[['FAC_NAME', 'Year of Suspected Closure']].head(10) 
first_10_closures

```


3. 
    a.

```{python}
# Step 1: Count the number of active hospitals in each ZIP code per year
active_hospitals_per_zip_year = all_years_data[all_years_data['PGM_TRMNTN_CD'] == 'A'] \
                                .groupby(['ZIP_CD', 'Year'])['PRVDR_NUM'] \
                                .nunique().reset_index(name='Active_Hospital_Count')

# Step 2: Merge the suspected closures with the active hospital counts to get the count for the year of suspected closure and the following year
closures_with_counts = closures_df.merge(
    active_hospitals_per_zip_year,
    left_on=['ZIP_CD', 'Year of Suspected Closure'],
    right_on=['ZIP_CD', 'Year'],
    how='left'
).rename(columns={'Active_Hospital_Count': 'Closure_Year_Count'})

closures_with_counts = closures_with_counts.merge(
    active_hospitals_per_zip_year,
    left_on=['ZIP_CD', 'Year of Suspected Closure + 1'],
    right_on=['ZIP_CD', 'Year'],
    how='left'
).rename(columns={'Active_Hospital_Count': 'Following_Year_Count'})

# Step 3: Filter suspected closures where the count of active hospitals does not decrease in the following year
potential_mergers = closures_with_counts[
    (closures_with_counts['Following_Year_Count'] >= closures_with_counts['Closure_Year_Count'])
]

# Output the number of potential mergers
num_potential_mergers = potential_mergers.shape[0]
num_potential_mergers

```

    b.
```{python}
# Step 1: Remove potential mergers from the suspected closures
corrected_closures = closures_df[~closures_df['PRVDR_NUM'].isin(potential_mergers['PRVDR_NUM'])]

# Step 2: Count the remaining hospitals in the corrected closures list
num_corrected_closures = corrected_closures.shape[0]
num_corrected_closures

```

    c.

```{python}
# Step 1: Sort the corrected closures DataFrame by 'Facility Name'
sorted_corrected_closures = corrected_closures.sort_values(by='FAC_NAME')

# Step 2: Display the first 10 rows of the sorted DataFrame
first_10_corrected_closures = sorted_corrected_closures[['FAC_NAME', 'Year of Suspected Closure']].head(10)

# Display the results
first_10_corrected_closures

```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are:

    - .dbf: The dBASE table that stores the attribute information of features; data is stored in an array with multiple records and fields (has attribute info)
    - .prj: The file that stores the coordinate system information 
    - .shp: The main file that stores the feature geometry (points, lines, polygons)
    - .shx: The index file that stores the index of the feature geometry (has positional index)
    - .xml: file format that stores information about the shapefile

    # Attribution: https://desktop.arcgis.com/en/arcmap/latest/manage-data/shapefiles/shapefile-file-extensions.htm
    
    b. The size for each of the files is given in the output below.

```{python}
import os

# define the folder path
folder_path = '/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/gz_2010_us_860_00_500k'

# get file size
for file_name in os.listdir(folder_path):
    file_path = os.path.join(folder_path, file_name)
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert size to MB
    print(f"File Name: {file_name}, File Size: {file_size_mb:.2f} MB")
```

2. The number of hospitals per zip code is given in the output below.

```{python}
import geopandas as gpd
import matplotlib.pyplot as plt

# loading .shp file
filepath = "/Users/tarini_dewan/Desktop/UChicago/Python_2/problem-set-4-shreya-and-tarini/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
texas_df = gpd.read_file(filepath)

# filtering data to Texas zip codes
texas_df['Texas'] = texas_df['ZCTA5'].apply(lambda x: 1 if x.startswith(('75', '76', '77', '78', '79')) else 0)
texas_df = texas_df[texas_df['Texas'] == 1]

# convert zip codes in both datasets to string
texas_df['ZCTA5'] = texas_df['ZCTA5'].astype(str)
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].fillna(0) # account for NAs in pos2016 zip code data
pos2016['ZIP_CD'] = pos2016['ZIP_CD'].astype(int).astype(str)

# merge both datasets by zip code
hosp_tex_2016 = pd.merge(texas_df, pos2016, how = 'inner', left_on = 'ZCTA5', right_on = 'ZIP_CD')

# number of hospitals in Texas per zip code 2016
print(hosp_tex_2016['ZCTA5'].value_counts())

# 
hosp_per_zip = hosp_tex_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# merge with spatial geometry data for zip codes
hospitals_geo = hosp_tex_2016[['ZIP_CD', 'geometry']].drop_duplicates().merge(
    hosp_per_zip, on='ZIP_CD', how='left').fillna(0)

# choropleth of hospitals by zip code
plt.figure(figsize=(20, 16), dpi = 1000)
hospitals_geo.plot(column='hospital_count', cmap='OrRd', legend=True, edgecolor='black').set_axis_off()
plt.title("Number of Hospitals per ZIP Code in Texas 2016")
plt.show()
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
